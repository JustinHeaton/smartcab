{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Implement a basic driving agent**\n",
    "\n",
    "To begin, your only task is to get the smartcab to move around in the environment. At this point, you will not be concerned with any sort of optimal driving policy. Note that the driving agent is given the following information at each intersection:\n",
    "\n",
    "The next waypoint location relative to its current location and heading.\n",
    "The state of the traffic light at the intersection and the presence of oncoming vehicles from other directions.\n",
    "The current time left from the allotted deadline.\n",
    "To complete this task, simply have your driving agent choose a random action from the set of possible actions (None, 'forward', 'left', 'right') at each intersection, disregarding the input information above. Set the simulation deadline enforcement, enforce_deadline to False and observe how it performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Observe what you see with the agent's behavior as it takes random actions. Does the smartcab eventually make it to the destination? Are there any other interesting observations to note?\n",
    "\n",
    "*In order to make the driving agent move around the environment, I gave it simple instructions to choose randomly from a list of all possible actions.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Identify all possible actions\n",
    "self.actions = ['right','left','forward',None]\n",
    "#Choose randomly from the list of actions\n",
    "action = random.choice(self.actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*With these instructions to choose random actions from the list self.actions, the agent was able to move throughout the environment but there was no rhyme or reason to its movements. The agent did occasionally reach the destination, but this was just by chance, and the agent was unable to learn from it's past movements which it was either rewarded or penalized for.*\n",
    "\n",
    "*My method for measuring the success of the agent is to track the number of successful trials, and the net reward gained out of 100 trials.*\n",
    "\n",
    "*With the enforce_deadline parameter set to false, and all actions chosen at random, the agent had 19 successful trials out of 100, and a net reward of -242.5, meaning that it did reach the destination 19 times, but it was penalized more that it was rewarded.*\n",
    "\n",
    "*These kind of poor results are to be expected because the car is moving around the environment with no regard for traffic laws or other vehicles on the road.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: Inform the driving agent**\n",
    "\n",
    "Now that your driving agent is capable of moving around in the environment, your next task is to identify a set of states that are appropriate for modeling the smartcab and environment. The main source of state variables are the current inputs at the intersection, but not all may require representation. You may choose to explicitly define states, or use some combination of inputs as an implicit state. At each time step, process the inputs and update the agent's current state using the self.state variable. Continue with the simulation deadline enforcement enforce_deadline being set to False, and observe how your driving agent now reports the change in state as the simulation progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What states have you identified that are appropriate for modeling the smartcab and environment? Why do you believe each of these states to be appropriate for this problem?\n",
    "\n",
    "*The state is determined by the agent by sensing current inputs when the car arrives at an intersection,and is updated after each action is completed. The relevant inputs which should influence the decision of the agent at a given state are the following*\n",
    "\n",
    "* The condition of the traffic light (red or green)\n",
    "* Whether or not there is oncoming traffic from straight ahead to which the agent should yield right of way\n",
    "* Whether or not there is traffic coming from the left to which the agent should yield right of way \n",
    "* The next waypoint which tells the agent which action will move it closer to the destination.*\n",
    "\n",
    "*I have decided not to include the input of traffic from the right because in US traffic laws there are no situations in which a car must yield to traffic from the right.*\n",
    "\n",
    "*I have also decided to exclude the input of deadline because I wouldn't want the agent to violate traffic laws in order to reach the destination before the deadline.*\n",
    "\n",
    "*Based on these inputs, the agent was given instructions on which actions are legal in the current state, and if a move towards the next waypoint is also a legal action, the agent was instructed to take that move. At this point the only two valid options in any state are to move towards the next waypoint, or to stay where it is and wait for the conditions to change*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "legal_actions = []\n",
    "        if inputs['light'] == 'red':\n",
    "            if inputs['oncoming'] != 'left' and inputs['left'] != 'forward':\n",
    "                legal_actions = ['right', None]\n",
    "        else:\n",
    "            if inputs['oncoming'] == 'forward':\n",
    "                legal_actions = [ 'forward','right']\n",
    "            else:\n",
    "                legal_actions = ['right','forward', 'left']\n",
    "#Choose an action from the list of legal actions\n",
    "if legal_actions != []:\n",
    "    if self.next_waypoint in legal_actions:\n",
    "        action = self.next_waypoint # Move in direction of destination\n",
    "    else:\n",
    "        action = None \n",
    "else: action = None #There are no legal actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3: Implement a Q-learning driving agent**\n",
    "\n",
    "With your driving agent being capable of interpreting the input information and having a mapping of environmental states, your next task is to implement the Q-Learning algorithm for your driving agent to choose the best action at each time step, based on the Q-values for the current state and action. Each action taken by the smartcab will produce a reward which depends on the state of the environment. The Q-Learning driving agent will need to consider these rewards when updating the Q-values. Once implemented, set the simulation deadline enforcement enforce_deadline to True. Run the simulation and observe how the smartcab moves about the environment in each trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? Why is this behavior occurring?\n",
    "\n",
    "*When the Q-learning algorithm is implemented, every state and action pair that the agent takes is stored along with it's associated reward. The agent is then able to learn based on its previous experience and make an informed decision when it reaches a state which it has already exerienced.*\n",
    "\n",
    "*Now as we begin to learn what reward results from taking a specific action in a specific state, we are able to further inform the driving agent by passing it a best action rather than just a random legal action.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.Q = {} #Dictionary of Q values\n",
    "self.Q[(state, action)]= reward #Pass information to self.Q\n",
    "\n",
    "#Calculate best action\n",
    "max_Q = -float(\"inf\")\n",
    "        best_action = None\n",
    "        for action in legal_actions:\n",
    "            Q = self.Q[(state, action)]\n",
    "            if Q > max_Q:\n",
    "                max_Q = Q\n",
    "                best_action = action\n",
    "                \n",
    "#Instruct the agent\n",
    "if best_action != None:\n",
    "    action = best_action\n",
    "else:\n",
    "    if legal_actions != []:\n",
    "        if self.next_waypoint in legal_actions:\n",
    "            action = self.next_waypoint # Move in direction of destination\n",
    "        else:\n",
    "            action = None\n",
    "    else: action = None #There are no legal actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now with the driving agent being informed to take the best action rather than just taking a random legal action, it is beginning to favor moves in the direction of the destination, resulting in a high net reward and a high rate of success. The agent is now following traffic laws, reaching the destination almost every time, and avoiding penalties (only 17 moves were made which resulted in penalties).*\n",
    "\n",
    "*With the enforce_deadline parameter now set to True, the car reached the destination 99 times out of 100, racked up 2,217.5 net reward points, and had 17 penalties.*\n",
    "\n",
    "*These numbers are way better than what we saw before implementing the Q-learning algorithm, which is to be expected because now the driving agent is making informed decisions. Before Q-learning the agent was gathering a lot of information, but it was not remembering any of it, and not learning anything.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4: Improve the Q-learning driving agent**\n",
    "\n",
    "Your final task for this project is to enhance your driving agent so that, after sufficient training, the smartcab is able to reach the destination within the allotted time safely and efficiently. Parameters in the Q-Learning algorithm, such as the learning rate (alpha), the discount factor (gamma) and the exploration rate (epsilon) all contribute to the driving agent’s ability to learn the best action for each state. To improve on the success of your smartcab:\n",
    "\n",
    "Set the number of trials, n_trials, in the simulation to 100.\n",
    "Run the simulation with the deadline enforcement enforce_deadline set to True (you will need to reduce the update delay update_delay and set the display to False).\n",
    "Observe the driving agent’s learning and smartcab’s success rate, particularly during the later trials.\n",
    "Adjust one or several of the above parameters and iterate this process.\n",
    "This task is complete once you have arrived at what you determine is the best combination of parameters required for your driving agent to learn successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Report the different values for the parameters tuned in your basic implementation of Q-Learning. For which set of parameters does the agent perform best? How well does the final driving agent perform?\n",
    "\n",
    "*I will first adjust my Q-learning algorithm by implementing an alpha value, or learning rate, and measure it's impact on the success rate of the driving agent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Implementation of the learning rate as self.alpha\n",
    "self.Q[(state, action)]= self.alpha * reward + (1 - self.alpha) * self.Q[(state, action)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results from implementing self.alpha**\n",
    "\n",
    "self.alpha = 0.5 - 100% success rate, 2249 net reward\n",
    "\n",
    "self.alpha = 0.75 - 100% success rate, 2173 net reward\n",
    "\n",
    "self.alpha = 0.9 - 99% success rate, 2,257.5 net reward\n",
    "\n",
    "*At this point there wasn't much room for further improvement but with alpha rates of 0.5 and 0.75 the result was 100% success.* \n",
    "\n",
    "*Next I will test whether or not implementing an exploration rate (epsilon) will have any additional impact.*\n",
    "\n",
    "*An epsilon value works by informing the agent to to occasionally choose a random action rather than always taking the best action. The value of this is that a Q-learning algorithm can only be perfect if the driving agent enters every possible state, so sometimes the agent has to go off track to explore more of the environment. This is known as the exploitation-exploration trade-off.*\n",
    "\n",
    "*I added the following code to implement epsilon in to the agents action policy.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if random.random() < self.epsilon: # Take a random action (epsilon * 100)% of the time.\n",
    "    action = random.choice(self.actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results from implementing self.epsilon**\n",
    "\n",
    "self.epsilon = 0.2 - 99% success rate, 2,309 net reward, 29 penalties\n",
    "\n",
    "self.epsilon = .1 - 99% success rate, 2,192.5 net reward, 22 penalties\n",
    "\n",
    "self.epsilon = .05 - 99% success rate, 2,214.5 net reward, 14 penalties\n",
    "\n",
    "*Implementing epsilon does not appear to be leading to an improvement in results, but it might not be necessary to maintain a constant value of epsilon throughout all of the 100 trials. By adding the following additional code to decay the epsilon factor over time, and eliminate it altogether after the 50th trial, I will see if there is any additional improvement in the results.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.trials += 1 #Count the number of trials completed\n",
    "self.epsilon *= .99 #Decay the epsilon value so that as the model learns more, it will explore less\n",
    "if self.trials >= 50:\n",
    "    self.epsilon == 0 #Eliminate epsilon after 50 trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self.epsilon = 0.05 (plus the above parameters), self.alpha = 0.5 - 99% success rate, 2,258 net reward, 18 penalties\n",
    "\n",
    "*In this environment, implementing epsilon doesn't really help much because the agent was able to reach the destination every time without adding further exploration.*\n",
    "\n",
    "*There is one additional variable known as gamma, or the discount factor. Gamma works by determining the value of future states, a high value of gamma forces the agent to prefer higher long term rewards while a low gamma value makes the agent short sighted, focusing on near term rewards.*\n",
    "\n",
    "**Reference**\n",
    "\n",
    "https://en.wikipedia.org/wiki/Q-learning\n",
    "\n",
    "*Tests with implementing gamma in my Q-learning model didn't achieve a success rate higher than 97% so they will not be reported on and gamma will be left out of the model for now.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? How would you describe an optimal policy for this problem?\n",
    "\n",
    "*A perfect policy would be one that reaches the destination 100% of the time, in the fastest possible time, and incurs no penalties. I would say that my agent was very close to finding a perfect policy because in my final test with alpha and a decaying epsilon, the agent reached the destination 99 times out of 100, and in the final 10 trials it reached the destination 10 times and had 0 penalties. It appears that it may have figured out the optimal policy but it would require a test with a larger number of trials to be sure.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
